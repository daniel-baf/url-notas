{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d85e0c5f",
   "metadata": {},
   "source": [
    "# Análisis Exploratorio\n",
    "\n",
    "En esta sección, realizaremos un análisis inicial del conjunto de datos **UrbanSound8K**. Este análisis nos permitirá comprender la estructura y el contenido del banco de datos descargado. Antes de continuar, es importante asegurarnos de que se haya ejecutado correctamente el script **setup.sh** y que la carpeta ***UrbanSound8K*** esté ubicada dentro de la carpeta `data`.\n",
    "\n",
    "El conjunto de datos contiene información sobre archivos de audio clasificados en diferentes categorías de sonidos urbanos. A continuación, exploraremos las características principales del dataframe, incluyendo las columnas disponibles, el número de registros, los tipos de datos y las estadísticas descriptivas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b25caa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import numpy as np  # For numerical computations\n",
    "import matplotlib.pyplot as plt  # For plotting and visualization\n",
    "import librosa  # For audio processing\n",
    "from IPython.display import Audio  # For audio playback in Jupyter\n",
    "import glob  # For file path pattern matching\n",
    "from sklearn.preprocessing import LabelEncoder  # For encoding labels\n",
    "import tensorflow as tf  # For deep learning models\n",
    "import os  # For operating system interactions\n",
    "\n",
    "# Configure matplotlib to display plots inline in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470e81ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "# This code aims to verify if there are devices available for processing,\n",
    "# specifically GPUs, as this model is large and the data to process (audio) is also large.\n",
    "# Using a GPU can significantly accelerate the training of the model.\n",
    "print(\"Available devices:\")\n",
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8e8b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# Define the base path where the audio files and metadata are located\n",
    "FILES_PATH = \"./data/UrbanSound8K\"  # Path to the audio files\n",
    "\n",
    "# Read the CSV file containing the metadata of the UrbanSound8K dataset\n",
    "# This file includes information such as file name, sound class, duration, etc.\n",
    "dataframe_audios = pd.read_csv(\n",
    "    f\"{FILES_PATH}/metadata/UrbanSound8K.csv\"\n",
    ")  # UrbanSound8K provides this file upon downloading the dataset\n",
    "\n",
    "# Display the first few rows of the DataFrame to inspect its content\n",
    "dataframe_audios.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae8a3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column \"audio_path\" to the dataframe\n",
    "# This column contains the full file path for each audio file in the dataset.\n",
    "# The path is constructed using the base path (FILES_PATH), the fold number, and the slice file name.\n",
    "dataframe_audios[\"audio_path\"] = dataframe_audios.apply(\n",
    "    lambda row: os.path.join(\n",
    "        FILES_PATH, \"audio\", f\"fold{row['fold']}\", row[\"slice_file_name\"]\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Display the first few rows of the updated dataframe to verify the new column\n",
    "dataframe_audios.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eb6616",
   "metadata": {},
   "source": [
    "## Analisis del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8b4084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataframe\n",
    "# The `info()` method provides a concise summary of the dataframe, including the number of entries,\n",
    "# column names, non-null counts, and data types. This helps to understand the structure of the dataset.\n",
    "dataframe_audios.info()\n",
    "\n",
    "# Generate descriptive statistics for the dataframe\n",
    "# The `describe()` method computes summary statistics for numerical columns, such as count, mean,\n",
    "# standard deviation, min, max, and percentiles. This is useful for understanding the distribution\n",
    "# and range of the data.\n",
    "dataframe_audios.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2989d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display descriptive statistics for the dataframe\n",
    "# The `describe()` method provides a summary of statistics for both numerical and categorical columns.\n",
    "# By using `include=\"all\"`, it includes all columns, regardless of their data type.\n",
    "# This helps to understand the distribution, central tendency, and spread of the data,\n",
    "# as well as unique values and frequency for categorical columns.\n",
    "dataframe_audios.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1134a960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display unique classes and their counts\n",
    "# The `unique()` method is used to list all unique categories in the \"class\" column.\n",
    "# This provides an overview of the different sound classes present in the dataset.\n",
    "print(\"Categorías únicas:\", dataframe_audios[\"class\"].unique())\n",
    "\n",
    "# The `value_counts()` method counts the occurrences of each category in the \"class\" column.\n",
    "# This helps to understand the distribution of samples across different sound classes.\n",
    "print(\"Conteo por clase:\\n\", dataframe_audios[\"class\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6e5ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of classes in the dataset\n",
    "# The `value_counts()` method counts the occurrences of each unique value in the \"class\" column.\n",
    "# This provides an overview of how many samples belong to each sound class.\n",
    "# The `plot()` method is used to create a bar chart to visualize this distribution.\n",
    "\n",
    "dataframe_audios[\"class\"].value_counts().plot(kind=\"bar\", figsize=(10, 5))\n",
    "\n",
    "# Add a title and labels to the plot for better understanding\n",
    "plt.title(\"Distribución de Clases\")  # Title of the plot\n",
    "plt.ylabel(\"Cantidad\")  # Label for the y-axis\n",
    "plt.xlabel(\"Clase\")  # Label for the x-axis\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d0915a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the duration of each audio clip\n",
    "# The \"duration\" column is computed as the difference between the \"end\" and \"start\" columns,\n",
    "# which represent the start and end times of each audio clip in seconds.\n",
    "dataframe_audios[\"duration\"] = dataframe_audios[\"end\"] - dataframe_audios[\"start\"]\n",
    "\n",
    "# Plot the distribution of audio durations\n",
    "# A histogram is created to visualize the distribution of audio clip durations.\n",
    "# The x-axis represents the duration in seconds, and the y-axis represents the frequency of clips.\n",
    "dataframe_audios[\"duration\"].plot(kind=\"hist\", bins=50, figsize=(10, 5))\n",
    "\n",
    "# Add a title and labels to the plot for better understanding\n",
    "plt.title(\"Distribución de Duración\")  # Title of the plot\n",
    "plt.xlabel(\"Duración (segundos)\")  # Label for the x-axis\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54df94a4",
   "metadata": {},
   "source": [
    "### Probando la carga y visualización de un audio\n",
    "\n",
    "En esta sección, cargaremos un archivo de audio del conjunto de datos y lo visualizaremos. Para ello, utilizaremos las herramientas de procesamiento de audio disponibles en la biblioteca `librosa`. Además, acotaremos la visualización del audio según los tiempos de inicio y fin especificados en el dataframe `dataframe_audios`. Esto nos permitirá analizar de manera más detallada las características del audio seleccionado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8b80a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and play an audio file to ensure the audio data is correctly loaded and audible.\n",
    "# This step is crucial for verifying the integrity of the audio files in the dataset.\n",
    "\n",
    "# Select an audio file (e.g., the first row)\n",
    "audio_file_path = dataframe_audios[\"audio_path\"].iloc[0]  # Adjust the path if necessary\n",
    "audio_data, sample_rate = librosa.load(audio_file_path, sr=None)  # Load audio\n",
    "\n",
    "# Play the audio\n",
    "Audio(data=audio_data, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf93ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the waveform and spectrogram of the audio file\n",
    "# This step helps us understand the time-domain and frequency-domain characteristics of the audio data.\n",
    "\n",
    "# Plot the waveform\n",
    "plt.figure(figsize=(12, 4))\n",
    "librosa.display.waveshow(audio_data, sr=sample_rate)\n",
    "plt.title(\"Waveform of the Audio\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the spectrogram\n",
    "# The spectrogram provides a visual representation of the frequency content of the audio over time.\n",
    "spectrogram = librosa.amplitude_to_db(np.abs(librosa.stft(audio_data)), ref=np.max)\n",
    "plt.figure(figsize=(12, 4))\n",
    "librosa.display.specshow(spectrogram, sr=sample_rate, x_axis=\"time\", y_axis=\"log\")\n",
    "plt.colorbar(format=\"%+2.0f dB\")\n",
    "plt.title(\"Spectrogram\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Frequency (Hz)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbdbd37",
   "metadata": {},
   "source": [
    "# Preparacion de los datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eab31ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode class labels into numerical format\n",
    "# This step is necessary to prepare the data for machine learning models, which require numerical inputs.\n",
    "label_encoder = LabelEncoder()\n",
    "dataframe_audios[\"encoded_label\"] = label_encoder.fit_transform(\n",
    "    dataframe_audios[\"class\"]\n",
    ")  # Add a new column 'encoded_label' with numerical IDs for each class\n",
    "number_of_classes = len(\n",
    "    dataframe_audios[\"class\"].unique()\n",
    ")  # Calculate the total number of unique sound classes\n",
    "number_of_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5201418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets in a stratified manner\n",
    "# This ensures that the distribution of classes is preserved in both sets, which is crucial for balanced training and evaluation.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Features: paths to audio files\n",
    "audio_file_paths = dataframe_audios[\"audio_path\"]\n",
    "\n",
    "# Target: encoded class labels\n",
    "encoded_labels = dataframe_audios[\"encoded_label\"]\n",
    "\n",
    "# Perform the stratified split\n",
    "train_audio_paths, test_audio_paths, train_labels, test_labels = train_test_split(\n",
    "    audio_file_paths,\n",
    "    encoded_labels,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=encoded_labels,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92300a93",
   "metadata": {},
   "source": [
    "## Extraer las características de los audios\n",
    "\n",
    "En este módulo, nos enfocaremos en la extracción de características relevantes de los archivos de audio. Específicamente, utilizaremos los coeficientes cepstrales de frecuencia de Mel (MFCC) para representar cada archivo de audio como un tensor numérico. Los MFCC son ampliamente utilizados en el procesamiento de señales de audio, ya que capturan información importante sobre el contenido espectral del sonido, lo que los hace ideales para tareas de clasificación y reconocimiento de audio.\n",
    "\n",
    "El objetivo es implementar funciones que permitan calcular los MFCC de cada archivo de audio y garantizar que todos los tensores tengan una longitud fija, aplicando padding o recorte según sea necesario. Esto asegurará que los datos estén en un formato consistente para ser utilizados en modelos de aprendizaje profundo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac91667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc(audio_file_path, num_mfcc=13, max_padding_length=174):\n",
    "    \"\"\"\n",
    "    Extract Mel-frequency cepstral coefficients (MFCC) from an audio file.\n",
    "\n",
    "    This function ensures that all audio features have a fixed length by applying padding or truncation.\n",
    "    This is essential for preparing the data for machine learning models, which require consistent input dimensions.\n",
    "\n",
    "    Parameters:\n",
    "    - audio_file_path (str): Path to the audio file.\n",
    "    - num_mfcc (int): Number of MFCC features to extract.\n",
    "    - max_padding_length (int): Maximum length for padding or truncation.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: MFCC features with fixed dimensions.\n",
    "    \"\"\"\n",
    "    audio_signal, sample_rate = librosa.load(audio_file_path, sr=None)\n",
    "    mfcc_features = librosa.feature.mfcc(\n",
    "        y=audio_signal, sr=sample_rate, n_mfcc=num_mfcc\n",
    "    )\n",
    "\n",
    "    # Ensure fixed length by padding or truncating\n",
    "    if mfcc_features.shape[1] < max_padding_length:\n",
    "        mfcc_features = np.pad(\n",
    "            mfcc_features,\n",
    "            ((0, 0), (0, max_padding_length - mfcc_features.shape[1])),\n",
    "            mode=\"constant\",\n",
    "        )\n",
    "    else:\n",
    "        mfcc_features = mfcc_features[:, :max_padding_length]\n",
    "\n",
    "    return mfcc_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b2e2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply MFCC extraction to the training and testing datasets\n",
    "# This step is crucial to convert raw audio data into numerical features (MFCCs)\n",
    "# that can be used as input for the CNN model.\n",
    "\n",
    "X_train_mfcc_features = np.array([extract_mfcc(path) for path in train_audio_paths])\n",
    "X_test_mfcc_features = np.array([extract_mfcc(path) for path in test_audio_paths])\n",
    "\n",
    "# Add a channel dimension to the MFCC features\n",
    "# This is necessary because the CNN model expects input data with a channel dimension.\n",
    "X_train_mfcc_features = X_train_mfcc_features[\n",
    "    ..., np.newaxis\n",
    "]  # Shape: (samples, n_mfcc, max_pad_len, 1)\n",
    "X_test_mfcc_features = X_test_mfcc_features[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6bd5b8",
   "metadata": {},
   "source": [
    "# Modelo CNN\n",
    "\n",
    "En este módulo, implementaremos una red neuronal convolucional (CNN) para procesar los coeficientes cepstrales de frecuencia de Mel (MFCCs) extraídos de los archivos de audio. Las CNN son especialmente efectivas para tareas de clasificación de datos estructurados espacialmente, como imágenes o, en este caso, representaciones espectrales de audio.\n",
    "\n",
    "La arquitectura de la red incluye capas convolucionales para extraer características relevantes, capas de normalización para estabilizar el entrenamiento, y capas densas con regularización para evitar el sobreajuste. Finalmente, la red utiliza una capa de salida con activación softmax para clasificar los audios en las diferentes categorías de sonidos urbanos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bc0f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\"\"\"\n",
    "Define the CNN model for audio classification.\n",
    "\n",
    "Why we do this:\n",
    "- To classify audio samples into predefined categories using MFCC features.\n",
    "- Added BatchNormalization after each convolutional layer to stabilize and accelerate training.\n",
    "- Applied L2 regularization to dense layers to reduce overfitting.\n",
    "\"\"\"\n",
    "\n",
    "# Get the number of unique classes from the dataset\n",
    "num_classes = number_of_classes\n",
    "\n",
    "# Define the CNN model\n",
    "audio_classification_model = models.Sequential(\n",
    "    [\n",
    "        # Input layer\n",
    "        layers.Input(shape=(13, 174, 1)),  # Adjusted to match MFCC feature dimensions\n",
    "        # Convolutional blocks\n",
    "        layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        # Flatten + Dense layers\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01)),\n",
    "        layers.Dropout(0.5),\n",
    "        # Output layer\n",
    "        layers.Dense(\n",
    "            num_classes, activation=\"softmax\", kernel_regularizer=regularizers.l2(0.01)\n",
    "        ),  # One neuron per class\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "audio_classification_model.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Display the model summary\n",
    "audio_classification_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fdf551",
   "metadata": {},
   "source": [
    "**ENTRENAMIENTO DEL MODELO**\n",
    "\n",
    "En esta etapa, procederemos a entrenar el modelo de clasificación de audio utilizando los coeficientes cepstrales de frecuencia de Mel (MFCC) previamente extraídos. Este proceso puede tomar un tiempo considerable dependiendo de la capacidad de procesamiento de la GPU disponible y del tamaño del conjunto de datos. Asegúrate de contar con los recursos necesarios antes de iniciar el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54651029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the CNN model for audio classification\n",
    "# Why we do this:\n",
    "# - To optimize the model's weights using the training dataset.\n",
    "# - To evaluate the model's performance on the validation dataset during training.\n",
    "# - To monitor the learning process and adjust hyperparameters if necessary.\n",
    "\n",
    "history = audio_classification_model.fit(\n",
    "    X_train_mfcc_features,  # Training features (MFCCs)\n",
    "    train_labels,  # Training labels (encoded)\n",
    "    epochs=80,  # Number of training epochs (adjustable)\n",
    "    batch_size=32,  # Batch size for gradient updates\n",
    "    validation_data=(X_test_mfcc_features, test_labels),  # Validation data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6de399b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curva de aprendizaje\n",
    "plt.figure(figsize=(12, 8))  # Aumentar el tamaño de la figura\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Train Accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "\n",
    "# Agregar texto con el accuracy final\n",
    "final_val_accuracy = (\n",
    "    history.history[\"val_accuracy\"][-1] * 100\n",
    ")  # Último valor de val_accuracy\n",
    "plt.text(\n",
    "    len(history.history[\"val_accuracy\"]) - 1,  # Última época\n",
    "    history.history[\"val_accuracy\"][-10],  # Valor de val_accuracy\n",
    "    f\"{final_val_accuracy:.2f}%\",  # Texto con el porcentaje\n",
    "    fontsize=12,\n",
    "    color=\"red\",\n",
    "    ha=\"right\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)  # Añadir una cuadrícula para mayor precisión\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62937491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "# Why we do this:\n",
    "# - To measure the model's performance on unseen data.\n",
    "# - To calculate the test accuracy and loss for evaluation purposes.\n",
    "test_loss, test_accuracy = audio_classification_model.evaluate(\n",
    "    X_test_mfcc_features, test_labels\n",
    ")\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Generate a confusion matrix to analyze model predictions\n",
    "# Why we do this:\n",
    "# - To visualize the performance of the model in terms of correctly and incorrectly classified samples.\n",
    "# - To identify patterns of misclassification for further improvements.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "predicted_labels = np.argmax(\n",
    "    audio_classification_model.predict(X_test_mfcc_features), axis=1\n",
    ")\n",
    "confusion_matrix_result = confusion_matrix(test_labels, predicted_labels)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    confusion_matrix_result,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    xticklabels=label_encoder.classes_,\n",
    "    yticklabels=label_encoder.classes_,\n",
    ")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be54e649",
   "metadata": {},
   "source": [
    ": Guardar y cargar el modelo entrenado\n",
    "\n",
    "En esta sección, se explica cómo guardar el modelo entrenado en un formato recomendado por Keras y cómo cargarlo posteriormente para realizar predicciones o continuar con el entrenamiento. Guardar el modelo permite reutilizarlo sin necesidad de volver a entrenarlo, lo que ahorra tiempo y recursos computacionales. Además, se asegura la persistencia del modelo para futuros análisis o implementaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead12eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained audio classification model\n",
    "# Why we do this:\n",
    "# - To persist the trained model for future use without retraining.\n",
    "# - To ensure reproducibility and save computational resources.\n",
    "audio_classification_model.save(\"audio_classification_model.keras\")\n",
    "\n",
    "# Load the saved audio classification model\n",
    "# Why we do this:\n",
    "# - To reuse the trained model for predictions or further training.\n",
    "# - To validate that the saved model can be successfully restored.\n",
    "loaded_audio_classification_model = tf.keras.models.load_model(\n",
    "    \"audio_classification_model.keras\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9f9383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_audio_class(audio_file_path):\n",
    "    \"\"\"\n",
    "    Why we do this:\n",
    "    - To predict the class of a given audio file using the trained CNN model.\n",
    "    - To convert the MFCC features of the audio into a format compatible with the model.\n",
    "    - To decode the predicted class index back into the original class label.\n",
    "    \"\"\"\n",
    "    # Extract MFCC features and reshape for model input\n",
    "    mfcc_features = extract_mfcc(audio_file_path)[np.newaxis, ..., np.newaxis]\n",
    "\n",
    "    # Predict the class probabilities\n",
    "    predictions = audio_classification_model.predict(mfcc_features)\n",
    "\n",
    "    # Get the class index with the highest probability\n",
    "    predicted_class_index = np.argmax(predictions)\n",
    "\n",
    "    # Decode the class index to the original class label\n",
    "    predicted_class_label = label_encoder.inverse_transform([predicted_class_index])[0]\n",
    "\n",
    "    return predicted_class_label\n",
    "\n",
    "\n",
    "# Example usage\n",
    "example_audio_path = \"./data/UrbanSound8K/audio/fold5/100032-3-0-0.wav\"\n",
    "print(f\"Prediction: {predict_audio_class(example_audio_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f2c45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERIFICAR TODAS LAS FALLAS QUE TUVO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

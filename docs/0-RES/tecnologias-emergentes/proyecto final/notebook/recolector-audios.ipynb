{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d85e0c5f",
   "metadata": {},
   "source": [
    "# Análisis Exploratorio\n",
    "\n",
    "En esta sección, realizaremos un análisis inicial del conjunto de datos **UrbanSound8K**. Este análisis nos permitirá comprender la estructura y el contenido del banco de datos descargado. Antes de continuar, es importante asegurarnos de que se haya ejecutado correctamente el script **setup.sh** y que la carpeta ***UrbanSound8K*** esté ubicada dentro de la carpeta `data`.\n",
    "\n",
    "El conjunto de datos contiene información sobre archivos de audio clasificados en diferentes categorías de sonidos urbanos. A continuación, exploraremos las características principales del dataframe, incluyendo las columnas disponibles, el número de registros, los tipos de datos y las estadísticas descriptivas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b25caa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import numpy as np  # For numerical computations\n",
    "import matplotlib.pyplot as plt  # For plotting and visualizationa\n",
    "import librosa  # For audio processing\n",
    "from IPython.display import Audio  # For audio playback in Jupyter\n",
    "import glob  # For file path pattern matching\n",
    "from sklearn.preprocessing import LabelEncoder  # For encoding labels\n",
    "import tensorflow as tf  # For deep learning models\n",
    "import os  # For operating system interactions\n",
    "\n",
    "# Configure matplotlib to display plots inline in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470e81ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "# This code aims to verify if there are devices available for processing,\n",
    "# specifically GPUs, as this model is large and the data to process (audio) is also large.\n",
    "# Using a GPU can significantly accelerate the training of the model.\n",
    "print(\"Available devices:\")\n",
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8e8b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# Define the base path where the audio files and metadata are located\n",
    "FILES_PATH = \"./data/UrbanSound8K\"  # Path to the audio files\n",
    "ESC_50_PATH = \"./data/ESC-50-master\"  # Path to the ESC-50 dataset\n",
    "\n",
    "# Read the CSV file containing the metadata of the UrbanSound8K dataset\n",
    "# This file includes information such as file name, sound class, duration, etc.\n",
    "dataframe_audios = pd.read_csv(\n",
    "    f\"{FILES_PATH}/metadata/UrbanSound8K.csv\"\n",
    ")  # UrbanSound8K provides this file upon downloading the dataset\n",
    "\n",
    "dataframe_esc_50 = pd.read_csv(\n",
    "    f\"{ESC_50_PATH}/meta/esc50.csv\"\n",
    ")  # ESC-50 provides this file upon downloading the dataset\n",
    "\n",
    "# Display the first few rows of the DataFrame to inspect its content\n",
    "dataframe_audios.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c32596",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_esc_50.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0ccc79",
   "metadata": {},
   "source": [
    "### Combinando los DataFrames y manejando IDs y nombres distintos\n",
    "\n",
    "En esta sección, se realizará la combinación de los DataFrames `dataframe_audios` y `extra_dataframe`. Además, se gestionarán los conflictos relacionados con los IDs de clase (`classID`) y los nombres de las clases (`class`) para garantizar que no existan duplicados y que los datos estén correctamente alineados. Esto incluye ajustar los valores de `classID` en caso de que haya solapamientos y unificar los nombres de las clases para evitar inconsistencias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f46456",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_dataframe = dataframe_esc_50.copy()\n",
    "\n",
    "extra_dataframe.rename(\n",
    "    columns={\n",
    "        \"filename\": \"slice_file_name\",\n",
    "        \"src_file\": \"fsID\",\n",
    "        \"category\": \"class\",\n",
    "        \"esc10\": \"salience\",\n",
    "        \"target\": \"classID\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "extra_dataframe[\"start\"] = 0\n",
    "extra_dataframe[\"end\"] = np.nan\n",
    "extra_dataframe.drop(columns=[\"take\"], inplace=True)\n",
    "extra_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae8a3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column \"audio_path\" to the dataframe\n",
    "# This column contains the full file path for each audio file in the dataset.\n",
    "# The path is constructed using the base path (FILES_PATH), the fold number, and the slice file name.\n",
    "dataframe_audios[\"audio_path\"] = dataframe_audios.apply(\n",
    "    lambda row: os.path.join(\n",
    "        FILES_PATH, \"audio\", f\"fold{row['fold']}\", row[\"slice_file_name\"]\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "extra_dataframe[\"audio_path\"] = extra_dataframe.apply(\n",
    "    lambda row: os.path.join(ESC_50_PATH, \"audio\", row[\"slice_file_name\"]), axis=1\n",
    ")\n",
    "\n",
    "# Display the first few rows of the updated dataframe to verify the new column\n",
    "dataframe_audios.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e4314d",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3862edd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the dataframes, but to handle the classID column, we need to ensure that the class names are consistent we will increase the extra_dataframe classID by the max classID of the dataframe_audios\n",
    "# This is done to avoid any conflicts in class IDs when merging the two dataframes.\n",
    "# The classID of the extra_dataframe is adjusted by adding the maximum classID from the dataframe_audios.\n",
    "# This ensures that the class IDs from both datasets are unique and do not overlap.\n",
    "max_class_id = dataframe_audios[\"classID\"].max()\n",
    "extra_dataframe[\"classID\"] = extra_dataframe[\"classID\"] + max_class_id + 1\n",
    "\n",
    "# merging the two dataframes\n",
    "# The two dataframes are concatenated into a single dataframe called \"dataframe\".\n",
    "# This combined dataframe will be used for further processing and model training.\n",
    "dataframe_audios = pd.concat([dataframe_audios, extra_dataframe], ignore_index=True)\n",
    "# Display the first few rows of the merged dataframe to verify the result\n",
    "dataframe_audios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f251c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import process\n",
    "\n",
    "# Lista base de clases destino (deduplicada y limpia)\n",
    "target_classes = [\n",
    "    \"dog\", \"cat\", \"engine\", \"baby\", \"door\", \"clock\", \"typing\", \"click\", \"voice\",\n",
    "    \"gun\", \"air_conditioner\", \"car\", \"birds\", \"vacuum\", \"thunder\", \"can\", \"crow\",\n",
    "    \"clapping\", \"fireworks\", \"chainsaw\", \"airplane\", \"water\", \"train\", \"sheep\",\n",
    "    \"church_bells\", \"wind\", \"footsteps\", \"frog\", \"cow\", \"brushing\", \"fire\",\n",
    "    \"helicopter\", \"drinking\", \"rain\", \"insects\", \"hen\", \"breathing\", \"saw\", \"glass\",\n",
    "    \"snoring\", \"toilet\", \"pig\", \"washing_machine\", \"sneeze\", \"rooster\", \"sea\", \"creak\",\n",
    "]\n",
    "\n",
    "# Función para mapear cada clase original a la más cercana\n",
    "def map_to_closest_class(original_class, target_classes, threshold=80):\n",
    "    match, score, _ = process.extractOne(original_class, target_classes)\n",
    "    if score >= threshold:\n",
    "        return match\n",
    "    else:\n",
    "        return original_class  # mantener si no hay buen match\n",
    "\n",
    "# Aplicar el mapeo de clases\n",
    "dataframe_audios[\"class_standardized\"] = dataframe_audios[\"class\"].apply(\n",
    "    lambda x: map_to_closest_class(x, target_classes)\n",
    ")\n",
    "\n",
    "# Crear un diccionario nuevo de classID para las clases estandarizadas\n",
    "class_mapping = {\n",
    "    class_name: idx for idx, class_name in enumerate(sorted(dataframe_audios[\"class_standardized\"].unique()))\n",
    "}\n",
    "\n",
    "# Asignar los nuevos classID basados en las clases estandarizadas\n",
    "dataframe_audios[\"classID\"] = dataframe_audios[\"class_standardized\"].map(class_mapping)\n",
    "\n",
    "# Reemplazar la columna original 'class' con la estandarizada\n",
    "dataframe_audios[\"class\"] = dataframe_audios[\"class_standardized\"]\n",
    "\n",
    "# Eliminar columna temporal\n",
    "dataframe_audios.drop(columns=[\"class_standardized\"], inplace=True)\n",
    "\n",
    "# (Opcional) Guardar resultado\n",
    "# dataframe_audios.to_csv(\"dataset_estandarizado.csv\", index=False)\n",
    "\n",
    "# Mostrar resumen\n",
    "print(\"Total de clases estandarizadas:\", len(class_mapping))\n",
    "print(\"Nuevos classID asignados:\", class_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eb6616",
   "metadata": {},
   "source": [
    "## Analisis del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8b4084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataframe\n",
    "# The `info()` method provides a concise summary of the dataframe, including the number of entries,\n",
    "# column names, non-null counts, and data types. This helps to understand the structure of the dataset.\n",
    "dataframe_audios.info()\n",
    "\n",
    "# Generate descriptive statistics for the dataframe\n",
    "# The `describe()` method computes summary statistics for numerical columns, such as count, mean,\n",
    "# standard deviation, min, max, and percentiles. This is useful for understanding the distribution\n",
    "# and range of the data.\n",
    "dataframe_audios.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2989d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display descriptive statistics for the dataframe\n",
    "# The `describe()` method provides a summary of statistics for both numerical and categorical columns.\n",
    "# By using `include=\"all\"`, it includes all columns, regardless of their data type.\n",
    "# This helps to understand the distribution, central tendency, and spread of the data,\n",
    "# as well as unique values and frequency for categorical columns.\n",
    "dataframe_audios.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1134a960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display unique classes and their counts\n",
    "# The `unique()` method is used to list all unique categories in the \"class\" column.\n",
    "# This provides an overview of the different sound classes present in the dataset.\n",
    "print(\"Categorías únicas:\", dataframe_audios[\"class\"].unique())\n",
    "\n",
    "# The `value_counts()` method counts the occurrences of each category in the \"class\" column.\n",
    "# This helps to understand the distribution of samples across different sound classes.\n",
    "print(\"Conteo por clase:\\n\", dataframe_audios[\"class\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6e5ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of classes in the dataset\n",
    "# The `value_counts()` method counts the occurrences of each unique value in the \"class\" column.\n",
    "# This provides an overview of how many samples belong to each sound class.\n",
    "# The `plot()` method is used to create a bar chart to visualize this distribution.\n",
    "\n",
    "dataframe_audios[\"class\"].value_counts().plot(kind=\"bar\", figsize=(10, 5))\n",
    "\n",
    "# Add a title and labels to the plot for better understanding\n",
    "plt.title(\"Distribución de Clases\")  # Title of the plot\n",
    "plt.ylabel(\"Cantidad\")  # Label for the y-axis\n",
    "plt.xlabel(\"Clase\")  # Label for the x-axis\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d0915a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the duration of each audio clip\n",
    "# The \"duration\" column is computed as the difference between the \"end\" and \"start\" columns,\n",
    "# which represent the start and end times of each audio clip in seconds.\n",
    "dataframe_audios[\"duration\"] = dataframe_audios[\"end\"] - dataframe_audios[\"start\"]\n",
    "\n",
    "# Plot the distribution of audio durations\n",
    "# A histogram is created to visualize the distribution of audio clip durations.\n",
    "# The x-axis represents the duration in seconds, and the y-axis represents the frequency of clips.\n",
    "dataframe_audios[\"duration\"].plot(kind=\"hist\", bins=50, figsize=(10, 5))\n",
    "\n",
    "# Add a title and labels to the plot for better understanding\n",
    "plt.title(\"Distribución de Duración\")  # Title of the plot\n",
    "plt.xlabel(\"Duración (segundos)\")  # Label for the x-axis\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d26a91c",
   "metadata": {},
   "source": [
    "Unificación de Clases Duplicadas\n",
    "En el conjunto de datos, existen clases con nombres similares o duplicados, como dog y dog_bark. Es importante identificar y unificar estos valores para evitar inconsistencias durante el entrenamiento y la evaluación del modelo. Este proceso garantiza que cada categoría represente una clase única y bien definida, mejorando así la calidad y la interpretabilidad de los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83c5e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_audios.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54df94a4",
   "metadata": {},
   "source": [
    "### Probando la carga y visualización de un audio\n",
    "\n",
    "En esta sección, cargaremos un archivo de audio del conjunto de datos y lo visualizaremos. Para ello, utilizaremos las herramientas de procesamiento de audio disponibles en la biblioteca `librosa`. Además, acotaremos la visualización del audio según los tiempos de inicio y fin especificados en el dataframe `dataframe_audios`. Esto nos permitirá analizar de manera más detallada las características del audio seleccionado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8b80a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and play an audio file to ensure the audio data is correctly loaded and audible.\n",
    "# This step is crucial for verifying the integrity of the audio files in the dataset.\n",
    "\n",
    "# Select an audio file (e.g., the first row)\n",
    "audio_file_path = dataframe_audios[\"audio_path\"].iloc[0]  # Adjust the path if necessary\n",
    "audio_data, sample_rate = librosa.load(audio_file_path, sr=None)  # Load audio\n",
    "\n",
    "# Play the audio\n",
    "Audio(data=audio_data, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf93ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the waveform and spectrogram of the audio file\n",
    "# This step helps us understand the time-domain and frequency-domain characteristics of the audio data.\n",
    "\n",
    "# Plot the waveform\n",
    "plt.figure(figsize=(12, 4))\n",
    "librosa.display.waveshow(audio_data, sr=sample_rate)\n",
    "plt.title(\"Waveform of the Audio\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the spectrogram\n",
    "# The spectrogram provides a visual representation of the frequency content of the audio over time.\n",
    "spectrogram = librosa.amplitude_to_db(np.abs(librosa.stft(audio_data)), ref=np.max)\n",
    "plt.figure(figsize=(12, 4))\n",
    "librosa.display.specshow(spectrogram, sr=sample_rate, x_axis=\"time\", y_axis=\"log\")\n",
    "plt.colorbar(format=\"%+2.0f dB\")\n",
    "plt.title(\"Spectrogram\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Frequency (Hz)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbdbd37",
   "metadata": {},
   "source": [
    "# Preparacion de los datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eab31ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode class labels into numerical format\n",
    "# This step is necessary to prepare the data for machine learning models, which require numerical inputs.\n",
    "label_encoder = LabelEncoder()\n",
    "dataframe_audios[\"encoded_label\"] = label_encoder.fit_transform(\n",
    "    dataframe_audios[\"class\"]\n",
    ")  # Add a new column 'encoded_label' with numerical IDs for each class\n",
    "number_of_classes = len(\n",
    "    dataframe_audios[\"class\"].unique()\n",
    ")  # Calculate the total number of unique sound classes\n",
    "number_of_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5201418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets in a stratified manner\n",
    "# This ensures that the distribution of classes is preserved in both sets, which is crucial for balanced training and evaluation.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Features: paths to audio files\n",
    "audio_file_paths = dataframe_audios[\"audio_path\"]\n",
    "\n",
    "# Target: encoded class labels\n",
    "encoded_labels = dataframe_audios[\"encoded_label\"]\n",
    "\n",
    "# Perform the stratified split\n",
    "train_audio_paths, test_audio_paths, train_labels, test_labels = train_test_split(\n",
    "    audio_file_paths,\n",
    "    encoded_labels,\n",
    "    test_size=0.3,\n",
    "    random_state=8,\n",
    "    stratify=encoded_labels,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92300a93",
   "metadata": {},
   "source": [
    "## Extraer las características de los audios\n",
    "\n",
    "En este módulo, nos enfocaremos en la extracción de características relevantes de los archivos de audio. Específicamente, utilizaremos los coeficientes cepstrales de frecuencia de Mel (MFCC) para representar cada archivo de audio como un tensor numérico. Los MFCC son ampliamente utilizados en el procesamiento de señales de audio, ya que capturan información importante sobre el contenido espectral del sonido, lo que los hace ideales para tareas de clasificación y reconocimiento de audio.\n",
    "\n",
    "El objetivo es implementar funciones que permitan calcular los MFCC de cada archivo de audio y garantizar que todos los tensores tengan una longitud fija, aplicando padding o recorte según sea necesario. Esto asegurará que los datos estén en un formato consistente para ser utilizados en modelos de aprendizaje profundo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fbb08f",
   "metadata": {},
   "source": [
    "### Normalizar los MFCCs\n",
    "\n",
    "En este paso, normalizamos las características MFCC para garantizar que todos los valores estén en una escala similar, lo que ayuda a mejorar el rendimiento y la estabilidad del modelo durante el entrenamiento.\n",
    "\n",
    "### Agregar características delta\n",
    "\n",
    "Las características delta representan la tasa de cambio de los MFCCs a lo largo del tiempo. Agregar estas características puede proporcionar información temporal adicional, lo que podría mejorar la capacidad del modelo para clasificar señales de audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac91667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc(audio_file_path, num_mfcc=13, max_padding_length=174, sr=22050):\n",
    "    \"\"\"\n",
    "    Extracts MFCCs with delta and delta-delta features from an audio file.\n",
    "    Returns an array of shape (39, max_padding_length) for each file.\n",
    "    \"\"\"\n",
    "    # 1. Load audio\n",
    "    audio_signal, sample_rate = librosa.load(audio_file_path, sr=sr)\n",
    "\n",
    "    # 2. Extract base MFCCs\n",
    "    mfcc_features = librosa.feature.mfcc(\n",
    "        y=audio_signal, sr=sample_rate, n_mfcc=num_mfcc, fmax=sample_rate / 2\n",
    "    )\n",
    "\n",
    "    # 3. Normalize\n",
    "    mfcc_features = librosa.util.normalize(mfcc_features)\n",
    "\n",
    "    # 4. Adjust length FIRST (padding or truncation)\n",
    "    if mfcc_features.shape[1] < max_padding_length:\n",
    "        mfcc_features = np.pad(\n",
    "            mfcc_features,\n",
    "            ((0, 0), (0, max_padding_length - mfcc_features.shape[1])),\n",
    "            mode=\"constant\",\n",
    "        )\n",
    "    else:\n",
    "        mfcc_features = mfcc_features[:, :max_padding_length]\n",
    "\n",
    "    # 5. NOW compute deltas (safe because all have max_padding_length frames)\n",
    "    delta_mfcc = librosa.feature.delta(mfcc_features)\n",
    "    delta2_mfcc = librosa.feature.delta(mfcc_features, order=2)\n",
    "\n",
    "    # Combine (13 MFCC + 13 delta + 13 delta-delta = 39)\n",
    "    combined_features = np.concatenate((mfcc_features, delta_mfcc, delta2_mfcc))\n",
    "\n",
    "    return combined_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6bd5b8",
   "metadata": {},
   "source": [
    "# Modelo CNN\n",
    "\n",
    "En este módulo, implementaremos una red neuronal convolucional (CNN) para procesar los coeficientes cepstrales de frecuencia de Mel (MFCCs) extraídos de los archivos de audio. Las CNN son especialmente efectivas para tareas de clasificación de datos estructurados espacialmente, como imágenes o, en este caso, representaciones espectrales de audio.\n",
    "\n",
    "La arquitectura de la red incluye capas convolucionales para extraer características relevantes, capas de normalización para estabilizar el entrenamiento, y capas densas con regularización para evitar el sobreajuste. Finalmente, la red utiliza una capa de salida con activación softmax para clasificar los audios en las diferentes categorías de sonidos urbanos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bc0f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models, regularizers\n",
    "\n",
    "# Configuration\n",
    "# Define the input shape for the model. It includes 39 features (13 MFCC + 13 delta + 13 delta-delta),\n",
    "# a fixed padding length of 174, and a single channel.\n",
    "input_shape = (39, 174, 1)\n",
    "\n",
    "# Define the number of output classes based on the dataset.\n",
    "num_classes = number_of_classes\n",
    "\n",
    "# Model Architecture\n",
    "# We are building a Convolutional Neural Network (CNN) for audio classification.\n",
    "# The architecture includes convolutional layers for feature extraction,\n",
    "# pooling layers for dimensionality reduction, and dense layers for classification.\n",
    "model = models.Sequential(\n",
    "    [\n",
    "        # Input layer\n",
    "        layers.Input(shape=input_shape),\n",
    "        # Convolutional Block 1\n",
    "        # Extract features using 32 filters of size (3, 3), followed by batch normalization,\n",
    "        # max pooling to reduce spatial dimensions, and dropout for regularization.\n",
    "        layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.3),\n",
    "        # Convolutional Block 2\n",
    "        # Use 64 filters of size (3, 3) for deeper feature extraction, followed by batch normalization,\n",
    "        # max pooling, and dropout for regularization.\n",
    "        layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.3),\n",
    "        # Convolutional Block 3\n",
    "        # Use 128 filters of size (3, 3) for even deeper feature extraction, followed by batch normalization,\n",
    "        # max pooling, and dropout for regularization.\n",
    "        layers.Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.3),\n",
    "        # Dense Layers\n",
    "        # Flatten the feature maps into a 1D vector, followed by a dense layer with 256 units\n",
    "        # and L2 regularization to prevent overfitting. Batch normalization and dropout are applied.\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        # Output Layer\n",
    "        # Use a dense layer with `num_classes` units and softmax activation for multi-class classification.\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Compilation\n",
    "# Compile the model with the Adam optimizer, sparse categorical crossentropy loss (suitable for integer labels),\n",
    "# and accuracy as the evaluation metric.\n",
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Summary\n",
    "# Display the model architecture and the number of trainable parameters.\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fdf551",
   "metadata": {},
   "source": [
    "**ENTRENAMIENTO DEL MODELO**\n",
    "\n",
    "En esta etapa, procederemos a entrenar el modelo de clasificación de audio utilizando los coeficientes cepstrales de frecuencia de Mel (MFCC) previamente extraídos. Este proceso puede tomar un tiempo considerable dependiendo de la capacidad de procesamiento de la GPU disponible y del tamaño del conjunto de datos. Asegúrate de contar con los recursos necesarios antes de iniciar el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d94e2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction\n",
    "X_train = np.array([extract_mfcc(path) for path in train_audio_paths])\n",
    "X_test = np.array([extract_mfcc(path) for path in test_audio_paths])\n",
    "\n",
    "# Add channel dimension\n",
    "X_train = X_train[..., np.newaxis]  # Shape: (samples, 39, 174, 1)\n",
    "X_test = X_test[..., np.newaxis]\n",
    "\n",
    "# Convert labels to numpy arrays\n",
    "y_train = np.array(train_labels)\n",
    "y_test = np.array(test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54651029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks for training\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6),\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True),\n",
    "]\n",
    "\n",
    "# Training\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=70,  # after training the previous model, i saw it established a plateau at 70 epochs\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6de399b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curva de aprendizaje\n",
    "plt.figure(figsize=(12, 8))  # Aumentar el tamaño de la figura\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Train Accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "\n",
    "# Agregar texto con el accuracy final\n",
    "final_val_accuracy = (\n",
    "    history.history[\"val_accuracy\"][-1] * 100\n",
    ")  # Último valor de val_accuracy\n",
    "plt.text(\n",
    "    len(history.history[\"val_accuracy\"]) - 1,  # Última época\n",
    "    history.history[\"val_accuracy\"][-10],  # Valor de val_accuracy\n",
    "    f\"{final_val_accuracy:.2f}%\",  # Texto con el porcentaje\n",
    "    fontsize=12,\n",
    "    color=\"red\",\n",
    "    ha=\"right\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)  # Añadir una cuadrícula para mayor precisión\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907698c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "# Why we do this:\n",
    "# - To measure the model's performance on unseen data.\n",
    "# - To calculate the test accuracy and loss for evaluation purposes.\n",
    "test_loss, test_accuracy = model.evaluate(\n",
    "    X_test,  # Changed from X_test_mfcc_features to X_test\n",
    "    y_test,  # Changed from test_labels to y_test (numpy array version)\n",
    ")\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f724c3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a confusion matrix to analyze model predictions\n",
    "# Why we do this:\n",
    "# - To visualize the performance of the model in terms of correctly and incorrectly classified samples.\n",
    "# - To identify patterns of misclassification for further improvements.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get predictions\n",
    "predicted_labels = np.argmax(\n",
    "    model.predict(X_test),  # Changed from X_test_mfcc_features to X_test\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Generate confusion matrix\n",
    "confusion_matrix_result = confusion_matrix(\n",
    "    y_test, predicted_labels\n",
    ")  # Changed from test_labels to y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62937491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el heatmap\n",
    "plt.figure(figsize=(20, 20))  # Tamaño más grande para que entren bien las etiquetas\n",
    "\n",
    "# Ensure class names are extracted from the label encoder\n",
    "class_names = label_encoder.classes_\n",
    "\n",
    "# Create the heatmap\n",
    "ax = sns.heatmap(\n",
    "    confusion_matrix_result,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"BuPu\",  # Cambiado a \"viridis\"\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names,\n",
    "    linewidths=0.5,\n",
    "    linecolor=\"grey\",\n",
    "    cbar_kws={\"label\": \"Number of Samples\"},\n",
    "    annot_kws={\"size\": 6, \"color\": \"black\", \"weight\": \"bold\"},  # Cambiado a negro\n",
    ")\n",
    "\n",
    "# Ajustar los textos\n",
    "ax.set_title(\"Confusion Matrix\", fontsize=20, pad=20)\n",
    "ax.set_xlabel(\"Predicted\", fontsize=20)\n",
    "ax.set_ylabel(\"Actual\", fontsize=20)\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, fontsize=8)\n",
    "ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=8)\n",
    "\n",
    "# Fondo del eje\n",
    "ax.set_facecolor(\"black\")\n",
    "\n",
    "# Layout para evitar recortes\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b08ccb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Crear matriz binaria de correctos/incorrectos\n",
    "correct_matrix = np.zeros_like(confusion_matrix_result)\n",
    "np.fill_diagonal(correct_matrix, 1)  # Marcamos la diagonal como correctos (1)\n",
    "binary_matrix = (\n",
    "    confusion_matrix_result * correct_matrix\n",
    ")  # Solo quedan los valores de la diagonal\n",
    "\n",
    "# Matriz de incorrectos (todo menos la diagonal)\n",
    "incorrect_matrix = confusion_matrix_result - binary_matrix\n",
    "\n",
    "# Porcentaje de correctos por clase\n",
    "total_samples = confusion_matrix_result.sum(axis=1)\n",
    "correct_percentage = np.diag(confusion_matrix_result) / total_samples * 100\n",
    "\n",
    "# Crear el heatmap\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "# Paleta de colores: verde para correcto, rojo para incorrecto\n",
    "cmap = sns.diverging_palette(120, 10, as_cmap=True)\n",
    "\n",
    "ax = sns.heatmap(\n",
    "    binary_matrix,  # Mostramos solo la matriz binaria\n",
    "    annot=confusion_matrix_result,  # Mostramos los valores originales como anotación\n",
    "    fmt=\"d\",\n",
    "    cmap=cmap,\n",
    "    center=0.5,\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names,\n",
    "    linewidths=0.5,\n",
    "    linecolor=\"grey\",\n",
    "    cbar_kws={\"label\": \"Correct (1) / Incorrect (0)\"},\n",
    "    annot_kws={\"size\": 6, \"color\": \"black\", \"weight\": \"bold\"},\n",
    "    mask=(binary_matrix == 0),  # Enmascaramos los incorrectos para mostrarlos en blanco\n",
    ")\n",
    "\n",
    "# Añadir porcentaje de acierto en el eje y\n",
    "yticklabels = [\n",
    "    f\"{name}\\n({perc:.1f}% correct)\"\n",
    "    for name, perc in zip(class_names, correct_percentage)\n",
    "]\n",
    "ax.set_yticklabels(yticklabels, rotation=0, fontsize=8)\n",
    "\n",
    "# Ajustar los textos\n",
    "ax.set_title(\"Correct vs Incorrect Predictions by Class\", fontsize=20, pad=20)\n",
    "ax.set_xlabel(\"Predicted\", fontsize=20)\n",
    "ax.set_ylabel(\"Actual (with accuracy %)\", fontsize=20)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, fontsize=8)\n",
    "\n",
    "# Fondo del eje\n",
    "ax.set_facecolor(\"lightgray\")\n",
    "\n",
    "# Layout para evitar recortes\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be54e649",
   "metadata": {},
   "source": [
    ": Guardar y cargar el modelo entrenado\n",
    "\n",
    "En esta sección, se explica cómo guardar el modelo entrenado en un formato recomendado por Keras y cómo cargarlo posteriormente para realizar predicciones o continuar con el entrenamiento. Guardar el modelo permite reutilizarlo sin necesidad de volver a entrenarlo, lo que ahorra tiempo y recursos computacionales. Además, se asegura la persistencia del modelo para futuros análisis o implementaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead12eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained audio classification model\n",
    "# Why we do this:\n",
    "# - To persist the trained model for future use without retraining.\n",
    "# - To ensure reproducibility and save computational resources.\n",
    "model.save(\"./model/audio_classification_model.keras\")\n",
    "\n",
    "# Load the saved audio classification model\n",
    "# Why we do this:\n",
    "# - To reuse the trained model for predictions or further training.\n",
    "# - To validate that the saved model can be successfully restored.\n",
    "loaded_audio_classification_model = tf.keras.models.load_model(\n",
    "    \"./model/audio_classification_model.keras\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9f9383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_audio_class(audio_file_path):\n",
    "    \"\"\"\n",
    "    Why we do this:\n",
    "    - To predict the class of a given audio file using the trained CNN model.\n",
    "    - To convert the MFCC features of the audio into a format compatible with the model.\n",
    "    - To decode the predicted class index back into the original class label.\n",
    "    \"\"\"\n",
    "    # Extract MFCC features and reshape for model input\n",
    "    mfcc_features = extract_mfcc(audio_file_path)[np.newaxis, ..., np.newaxis]\n",
    "\n",
    "    # Predict the class probabilities\n",
    "    predictions = model.predict(mfcc_features)\n",
    "\n",
    "    # Get the class index with the highest probability\n",
    "    predicted_class_index = np.argmax(predictions)\n",
    "\n",
    "    # Decode the class index to the original class label\n",
    "    predicted_class_label = label_encoder.inverse_transform([predicted_class_index])[0]\n",
    "\n",
    "    return predicted_class_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79810816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "\n",
    "def predict_and_save_results(\n",
    "    test_audio_paths,\n",
    "    test_labels,\n",
    "    label_encoder,\n",
    "    num_samples=14,\n",
    "    output_file=\"predictions.json\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Randomly selects audio files, makes predictions, and saves results to JSON.\n",
    "\n",
    "    Args:\n",
    "        test_audio_paths: DataFrame/Series of test audio paths\n",
    "        test_labels: Corresponding test labels\n",
    "        label_encoder: LabelEncoder instance\n",
    "        num_samples: Number of samples to test\n",
    "        output_file: Path to save JSON results\n",
    "    \"\"\"\n",
    "    # Randomly select audio files\n",
    "    random_indices = random.sample(range(len(test_audio_paths)), num_samples)\n",
    "    selected_audio_files = test_audio_paths.iloc[random_indices]\n",
    "    selected_labels = test_labels.iloc[random_indices]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i, audio_file in enumerate(selected_audio_files):\n",
    "        actual_label = label_encoder.inverse_transform([selected_labels.iloc[i]])[0]\n",
    "        predicted_label = predict_audio_class(audio_file)\n",
    "\n",
    "        # Create result dictionary\n",
    "        result = {\n",
    "            \"audio_path\": str(audio_file),\n",
    "            \"actual_label\": actual_label,\n",
    "            \"predicted_label\": predicted_label,\n",
    "            \"correct\": actual_label == predicted_label,\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "        # Print to console\n",
    "        print(f\"Audio File: {audio_file}\")\n",
    "        print(f\"Actual Label: {actual_label}\")\n",
    "        print(f\"Predicted Label: {predicted_label}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    # Save to JSON\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "    print(f\"\\nSaved prediction results to {output_file}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "# Usage\n",
    "prediction_results = predict_and_save_results(\n",
    "    test_audio_paths,\n",
    "    test_labels,\n",
    "    label_encoder,\n",
    "    num_samples=14,\n",
    "    output_file=\"./model/audio_predictions.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbdd22b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
